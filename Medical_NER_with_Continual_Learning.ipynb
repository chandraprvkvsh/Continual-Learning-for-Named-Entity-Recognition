{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8156232,
          "sourceType": "datasetVersion",
          "datasetId": 4824718
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4183.158288,
      "end_time": "2024-04-21T17:53:19.819927",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-04-21T16:43:36.661639",
      "version": "2.5.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "wDkptPYrZSG0",
        "papermill": {
          "duration": 0.01692,
          "end_time": "2024-04-21T16:43:39.700519",
          "exception": false,
          "start_time": "2024-04-21T16:43:39.683599",
          "status": "completed"
        },
        "tags": []
      },
      "id": "wDkptPYrZSG0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:43:39.735887Z",
          "iopub.status.busy": "2024-04-21T16:43:39.735247Z",
          "iopub.status.idle": "2024-04-21T16:43:56.741416Z",
          "shell.execute_reply": "2024-04-21T16:43:56.740229Z"
        },
        "id": "D2TVKtMNZSG3",
        "outputId": "bd6d0743-fc0e-403a-b1bd-6c82f1caa7d7",
        "papermill": {
          "duration": 17.02679,
          "end_time": "2024-04-21T16:43:56.744004",
          "exception": false,
          "start_time": "2024-04-21T16:43:39.717214",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Collecting seqeval\n\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\n\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n\nBuilding wheels for collected packages: seqeval\n\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=9775a394d043f05b4d8a5bc9fbd764d7f2a333440665d7d8a0a8267e9349a48a\n\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n\nSuccessfully built seqeval\n\nInstalling collected packages: seqeval\n\nSuccessfully installed seqeval-1.2.2\n"
        }
      ],
      "id": "D2TVKtMNZSG3"
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "command = 'huggingface-cli login --token \"your_token_here\"'\n",
        "os.system(command)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:43:56.785512Z",
          "iopub.status.busy": "2024-04-21T16:43:56.784575Z",
          "iopub.status.idle": "2024-04-21T16:43:58.294680Z",
          "shell.execute_reply": "2024-04-21T16:43:58.293812Z"
        },
        "papermill": {
          "duration": 1.532638,
          "end_time": "2024-04-21T16:43:58.296754",
          "exception": false,
          "start_time": "2024-04-21T16:43:56.764116",
          "status": "completed"
        },
        "tags": [],
        "id": "MS5h0zLYivHm",
        "outputId": "eec229fc-d289-47a8-c0b5-1bc5cf87e5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n\nToken is valid (permission: write).\n\nYour token has been saved to /root/.cache/huggingface/token\n\nLogin successful\n"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "MS5h0zLYivHm"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:43:58.333456Z",
          "iopub.status.busy": "2024-04-21T16:43:58.333110Z",
          "iopub.status.idle": "2024-04-21T16:44:07.232350Z",
          "shell.execute_reply": "2024-04-21T16:44:07.231524Z"
        },
        "id": "tqhDPjAOZSG6",
        "papermill": {
          "duration": 8.920095,
          "end_time": "2024-04-21T16:44:07.234693",
          "exception": false,
          "start_time": "2024-04-21T16:43:58.314598",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "tqhDPjAOZSG6"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:07.270637Z",
          "iopub.status.busy": "2024-04-21T16:44:07.270180Z",
          "iopub.status.idle": "2024-04-21T16:44:07.274159Z",
          "shell.execute_reply": "2024-04-21T16:44:07.273356Z"
        },
        "id": "3no8MdVHZSG7",
        "papermill": {
          "duration": 0.023719,
          "end_time": "2024-04-21T16:44:07.275973",
          "exception": false,
          "start_time": "2024-04-21T16:44:07.252254",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "3no8MdVHZSG7"
    },
    {
      "cell_type": "code",
      "source": [
        "g1=pd.read_excel(\"/kaggle/input/miimasa/G1.xlsx\")\n",
        "g2=pd.read_excel(\"/kaggle/input/miimasa/G2.xlsx\")\n",
        "g3=pd.read_excel(\"/kaggle/input/miimasa/G3.xlsx\")\n",
        "\n",
        "g1_g2 = pd.concat([g1, g2, g3]).drop_duplicates()\n",
        "all_g = pd.concat([g1, g2, g3]).drop_duplicates()"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:07.312217Z",
          "iopub.status.busy": "2024-04-21T16:44:07.311945Z",
          "iopub.status.idle": "2024-04-21T16:44:11.578472Z",
          "shell.execute_reply": "2024-04-21T16:44:11.577629Z"
        },
        "id": "NCGJs-bQZSG7",
        "papermill": {
          "duration": 4.287205,
          "end_time": "2024-04-21T16:44:11.581098",
          "exception": false,
          "start_time": "2024-04-21T16:44:07.293893",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "NCGJs-bQZSG7"
    },
    {
      "cell_type": "code",
      "source": [
        "g1.drop([\"Unnamed: 0\",\"ID\"], axis=1, inplace=True)\n",
        "g1.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.617760Z",
          "iopub.status.busy": "2024-04-21T16:44:11.616851Z",
          "iopub.status.idle": "2024-04-21T16:44:11.634217Z",
          "shell.execute_reply": "2024-04-21T16:44:11.633395Z"
        },
        "id": "D0Q-rWfdZSG8",
        "outputId": "bf91764d-bcc1-41a7-ca80-c2571f7ab2cc",
        "papermill": {
          "duration": 0.037135,
          "end_time": "2024-04-21T16:44:11.636169",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.599034",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8:16:chronic_disease,20:32:treatment</td>\n",
              "      <td>portal fibrosis by liver biopsy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22:34:treatment</td>\n",
              "      <td>Contra-indication to liver biopsy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,32:44:treatment,,</td>\n",
              "      <td>Have a stable weight since the liver biopsy wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26:38:treatment,</td>\n",
              "      <td>Subject agrees to have a liver biopsy performe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>,43:55:treatment,</td>\n",
              "      <td>Liver steatosis (on visual estimate or on live...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   tags  \\\n",
              "0  8:16:chronic_disease,20:32:treatment   \n",
              "1                       22:34:treatment   \n",
              "2                    ,32:44:treatment,,   \n",
              "3                      26:38:treatment,   \n",
              "4                     ,43:55:treatment,   \n",
              "\n",
              "                                                text  \n",
              "0                    portal fibrosis by liver biopsy  \n",
              "1                  Contra-indication to liver biopsy  \n",
              "2  Have a stable weight since the liver biopsy wa...  \n",
              "3  Subject agrees to have a liver biopsy performe...  \n",
              "4  Liver steatosis (on visual estimate or on live...  "
            ]
          },
          "metadata": {}
        }
      ],
      "id": "D0Q-rWfdZSG8"
    },
    {
      "cell_type": "code",
      "source": [
        "g2.drop([\"Unnamed: 0\",\"ID\"], axis=1, inplace=True)\n",
        "g2.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.672556Z",
          "iopub.status.busy": "2024-04-21T16:44:11.671950Z",
          "iopub.status.idle": "2024-04-21T16:44:11.682595Z",
          "shell.execute_reply": "2024-04-21T16:44:11.681612Z"
        },
        "id": "MAb9u9KrZSG8",
        "outputId": "25579f9e-83c6-4ab6-ef3c-ef634c606c84",
        "papermill": {
          "duration": 0.031046,
          "end_time": "2024-04-21T16:44:11.684669",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.653623",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34:68:cancer,72:104:cancer</td>\n",
              "      <td>Participants with a diagnosis of chronic lymph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26:60:cancer,61:93:cancer</td>\n",
              "      <td>Histologically confirmed chronic lymphocytic l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7:16:chronic_disease,22:56:chronic_disease</td>\n",
              "      <td>Known infection with human immunodeficiency vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21:30:chronic_disease,36:70:chronic_disease</td>\n",
              "      <td>Patients with known infection with human immun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15:49:chronic_disease,50:59:chronic_disease</td>\n",
              "      <td>Patients with human immunodeficiency virus (HI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tags  \\\n",
              "0                   34:68:cancer,72:104:cancer   \n",
              "1                    26:60:cancer,61:93:cancer   \n",
              "2   7:16:chronic_disease,22:56:chronic_disease   \n",
              "3  21:30:chronic_disease,36:70:chronic_disease   \n",
              "4  15:49:chronic_disease,50:59:chronic_disease   \n",
              "\n",
              "                                                text  \n",
              "0  Participants with a diagnosis of chronic lymph...  \n",
              "1  Histologically confirmed chronic lymphocytic l...  \n",
              "2  Known infection with human immunodeficiency vi...  \n",
              "3  Patients with known infection with human immun...  \n",
              "4  Patients with human immunodeficiency virus (HI...  "
            ]
          },
          "metadata": {}
        }
      ],
      "id": "MAb9u9KrZSG8"
    },
    {
      "cell_type": "code",
      "source": [
        "g3.drop([\"Unnamed: 0\",\"ID\"], axis=1, inplace=True)\n",
        "g3.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.721613Z",
          "iopub.status.busy": "2024-04-21T16:44:11.721313Z",
          "iopub.status.idle": "2024-04-21T16:44:11.731050Z",
          "shell.execute_reply": "2024-04-21T16:44:11.730164Z"
        },
        "id": "F3iRhh9hZSG9",
        "outputId": "b8e9a542-8868-4b77-e9f0-3fb037d4dd20",
        "papermill": {
          "duration": 0.030392,
          "end_time": "2024-04-21T16:44:11.733083",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.702691",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tags</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16:20:treatment,25:44:treatment</td>\n",
              "      <td>Current use of hemo- or peritoneal dialysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24:43:treatment,</td>\n",
              "      <td>Intention to change to peritoneal dialysis, or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27:52:treatment,58:66:treatment</td>\n",
              "      <td>Participants with ongoing anticoagulation trea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8:16:treatment</td>\n",
              "      <td>Use of warfarin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1:9:treatment</td>\n",
              "      <td>warfarin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              tags  \\\n",
              "0  16:20:treatment,25:44:treatment   \n",
              "1                 24:43:treatment,   \n",
              "2  27:52:treatment,58:66:treatment   \n",
              "3                   8:16:treatment   \n",
              "4                    1:9:treatment   \n",
              "\n",
              "                                                text  \n",
              "0        Current use of hemo- or peritoneal dialysis  \n",
              "1  Intention to change to peritoneal dialysis, or...  \n",
              "2  Participants with ongoing anticoagulation trea...  \n",
              "3                                    Use of warfarin  \n",
              "4                                           warfarin  "
            ]
          },
          "metadata": {}
        }
      ],
      "id": "F3iRhh9hZSG9"
    },
    {
      "cell_type": "code",
      "source": [
        "all_g.drop([\"Unnamed: 0\",\"ID\"], axis=1, inplace=True)\n",
        "all_g = all_g.reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.770828Z",
          "iopub.status.busy": "2024-04-21T16:44:11.770319Z",
          "iopub.status.idle": "2024-04-21T16:44:11.777055Z",
          "shell.execute_reply": "2024-04-21T16:44:11.776338Z"
        },
        "papermill": {
          "duration": 0.027919,
          "end_time": "2024-04-21T16:44:11.779056",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.751137",
          "status": "completed"
        },
        "tags": [],
        "id": "5bdAiGK9ivHo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5bdAiGK9ivHo"
    },
    {
      "cell_type": "code",
      "source": [
        "g1_g2.drop([\"Unnamed: 0\",\"ID\"], axis=1, inplace=True)\n",
        "g1_g2 = g1_g2.reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.815725Z",
          "iopub.status.busy": "2024-04-21T16:44:11.815206Z",
          "iopub.status.idle": "2024-04-21T16:44:11.821128Z",
          "shell.execute_reply": "2024-04-21T16:44:11.820425Z"
        },
        "papermill": {
          "duration": 0.026241,
          "end_time": "2024-04-21T16:44:11.823021",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.796780",
          "status": "completed"
        },
        "tags": [],
        "id": "H4ojEiuBivHo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "H4ojEiuBivHo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Representations of labels\n",
        "# Using lowerlimit-1 is giving more sound results\n",
        "\n",
        "print(g1.iloc[0,1][20-1:32])"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.859039Z",
          "iopub.status.busy": "2024-04-21T16:44:11.858541Z",
          "iopub.status.idle": "2024-04-21T16:44:11.863105Z",
          "shell.execute_reply": "2024-04-21T16:44:11.862293Z"
        },
        "id": "Fuma6chIZSG9",
        "outputId": "e6ee0951-2fcc-48a6-e005-8a856e49d6d1",
        "papermill": {
          "duration": 0.024674,
          "end_time": "2024-04-21T16:44:11.864973",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.840299",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "liver biopsy\n"
        }
      ],
      "id": "Fuma6chIZSG9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Functions**\n",
        "\n",
        "The function below are helper functions which will assist in pre-processing datasets on the go."
      ],
      "metadata": {
        "id": "x_HW_xqKZSG-",
        "papermill": {
          "duration": 0.01729,
          "end_time": "2024-04-21T16:44:11.899822",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.882532",
          "status": "completed"
        },
        "tags": []
      },
      "id": "x_HW_xqKZSG-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "\n",
        "entityset = [\n",
        "    \"treatment\",\n",
        "    \"chronic_disease\",\n",
        "    \"cancer\",\n",
        "    \"gender\",\n",
        "    \"pregnancy\",\n",
        "    \"allergy_name\",\n",
        "    \"contraception_consent\",\n",
        "    \"language_literacy\",\n",
        "    \"technology_access\",\n",
        "    \"ethnicity\",\n",
        "    \"attribute_clinical_variable\",\n",
        "    \"age\",\n",
        "    \"body_mass_index\",\n",
        "    \"limit_upper_bound\",\n",
        "    \"lower_bound\"\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.936871Z",
          "iopub.status.busy": "2024-04-21T16:44:11.936540Z",
          "iopub.status.idle": "2024-04-21T16:44:11.941343Z",
          "shell.execute_reply": "2024-04-21T16:44:11.940461Z"
        },
        "id": "NWIFcVyCZSG-",
        "papermill": {
          "duration": 0.025601,
          "end_time": "2024-04-21T16:44:11.943171",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.917570",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "NWIFcVyCZSG-"
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {}\n",
        "\n",
        "# Assigning O to 0\n",
        "label_dict['O'] = 0\n",
        "\n",
        "# Assigning unique values for B-entitytype and I-entitytype\n",
        "for idx, label in enumerate(entityset, start=1):\n",
        "    label_dict[f'B-{label}'] = idx\n",
        "    label_dict[f'I-{label}'] = idx + len(entityset)\n",
        "\n",
        "print(label_dict)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:11.979944Z",
          "iopub.status.busy": "2024-04-21T16:44:11.979327Z",
          "iopub.status.idle": "2024-04-21T16:44:11.984523Z",
          "shell.execute_reply": "2024-04-21T16:44:11.983746Z"
        },
        "id": "928L8OpyZSG-",
        "outputId": "d21ff28e-db4b-4ba2-affa-3a4a8634d387",
        "papermill": {
          "duration": 0.025881,
          "end_time": "2024-04-21T16:44:11.986542",
          "exception": false,
          "start_time": "2024-04-21T16:44:11.960661",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'O': 0, 'B-treatment': 1, 'I-treatment': 16, 'B-chronic_disease': 2, 'I-chronic_disease': 17, 'B-cancer': 3, 'I-cancer': 18, 'B-gender': 4, 'I-gender': 19, 'B-pregnancy': 5, 'I-pregnancy': 20, 'B-allergy_name': 6, 'I-allergy_name': 21, 'B-contraception_consent': 7, 'I-contraception_consent': 22, 'B-language_literacy': 8, 'I-language_literacy': 23, 'B-technology_access': 9, 'I-technology_access': 24, 'B-ethnicity': 10, 'I-ethnicity': 25, 'B-attribute_clinical_variable': 11, 'I-attribute_clinical_variable': 26, 'B-age': 12, 'I-age': 27, 'B-body_mass_index': 13, 'I-body_mass_index': 28, 'B-limit_upper_bound': 14, 'I-limit_upper_bound': 29, 'B-lower_bound': 15, 'I-lower_bound': 30}\n"
        }
      ],
      "id": "928L8OpyZSG-"
    },
    {
      "cell_type": "code",
      "source": [
        "label2id=label_dict\n",
        "print(label2id)\n",
        "\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "print(id2label)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.022436Z",
          "iopub.status.busy": "2024-04-21T16:44:12.022145Z",
          "iopub.status.idle": "2024-04-21T16:44:12.026787Z",
          "shell.execute_reply": "2024-04-21T16:44:12.025977Z"
        },
        "id": "h1GT0h8nZSG_",
        "outputId": "8f4c92cb-3603-4863-b886-2dab56e06162",
        "papermill": {
          "duration": 0.024727,
          "end_time": "2024-04-21T16:44:12.028731",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.004004",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'O': 0, 'B-treatment': 1, 'I-treatment': 16, 'B-chronic_disease': 2, 'I-chronic_disease': 17, 'B-cancer': 3, 'I-cancer': 18, 'B-gender': 4, 'I-gender': 19, 'B-pregnancy': 5, 'I-pregnancy': 20, 'B-allergy_name': 6, 'I-allergy_name': 21, 'B-contraception_consent': 7, 'I-contraception_consent': 22, 'B-language_literacy': 8, 'I-language_literacy': 23, 'B-technology_access': 9, 'I-technology_access': 24, 'B-ethnicity': 10, 'I-ethnicity': 25, 'B-attribute_clinical_variable': 11, 'I-attribute_clinical_variable': 26, 'B-age': 12, 'I-age': 27, 'B-body_mass_index': 13, 'I-body_mass_index': 28, 'B-limit_upper_bound': 14, 'I-limit_upper_bound': 29, 'B-lower_bound': 15, 'I-lower_bound': 30}\n\n{0: 'O', 1: 'B-treatment', 16: 'I-treatment', 2: 'B-chronic_disease', 17: 'I-chronic_disease', 3: 'B-cancer', 18: 'I-cancer', 4: 'B-gender', 19: 'I-gender', 5: 'B-pregnancy', 20: 'I-pregnancy', 6: 'B-allergy_name', 21: 'I-allergy_name', 7: 'B-contraception_consent', 22: 'I-contraception_consent', 8: 'B-language_literacy', 23: 'I-language_literacy', 9: 'B-technology_access', 24: 'I-technology_access', 10: 'B-ethnicity', 25: 'I-ethnicity', 11: 'B-attribute_clinical_variable', 26: 'I-attribute_clinical_variable', 12: 'B-age', 27: 'I-age', 13: 'B-body_mass_index', 28: 'I-body_mass_index', 14: 'B-limit_upper_bound', 29: 'I-limit_upper_bound', 15: 'B-lower_bound', 30: 'I-lower_bound'}\n"
        }
      ],
      "id": "h1GT0h8nZSG_"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_words(row):\n",
        "    tags = row['tags'].split(',')  # Split the tags by comma\n",
        "    extracted_words = []\n",
        "    if isinstance(row['text'], str)==False:\n",
        "        return\n",
        "    for tag in tags:\n",
        "        tag_elements = tag.split(':')\n",
        "        if len(tag_elements) != 3:\n",
        "            # Skip this tag and continue with the next one\n",
        "            continue\n",
        "        start, end, label = tag_elements\n",
        "        start = int(start)\n",
        "        end = int(end)\n",
        "        extracted_word = row['text'][start-1:end]  # Extract word based on start and end\n",
        "\n",
        "        if extracted_word and not extracted_word[-1].isalpha():\n",
        "            extracted_word = extracted_word[:-1]\n",
        "\n",
        "        extracted_words.append({extracted_word:label})\n",
        "    return extracted_words"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.065160Z",
          "iopub.status.busy": "2024-04-21T16:44:12.064594Z",
          "iopub.status.idle": "2024-04-21T16:44:12.071134Z",
          "shell.execute_reply": "2024-04-21T16:44:12.070349Z"
        },
        "id": "fT_uEXoSZSG_",
        "papermill": {
          "duration": 0.026845,
          "end_time": "2024-04-21T16:44:12.073163",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.046318",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "fT_uEXoSZSG_"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to generate text ids\n",
        "def generate_text_ids(text):\n",
        "    # Split text into words, including words separated by special characters\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "# Function to generate label ids\n",
        "def generate_label_ids(labels, text_ids, entityset):\n",
        "    if labels is None:\n",
        "        return ['O'] * len(text_ids)  # Return default label list if labels is None\n",
        "\n",
        "    label_ids = ['O'] * len(text_ids)\n",
        "    for label_dict in labels:\n",
        "        for entity, entity_type in label_dict.items():\n",
        "            entity_words = re.findall(r'\\b\\w+\\b', entity)\n",
        "            entity_start_index = -1\n",
        "            for i in range(len(text_ids) - len(entity_words) + 1):\n",
        "                if text_ids[i:i+len(entity_words)] == entity_words:\n",
        "                    entity_start_index = i\n",
        "                    break\n",
        "            if entity_start_index != -1:\n",
        "                label_ids[entity_start_index] = 'B-' + entity_type\n",
        "                for i in range(1, len(entity_words)):\n",
        "                    label_ids[entity_start_index + i] = 'I-' + entity_type\n",
        "    return label_ids\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.110056Z",
          "iopub.status.busy": "2024-04-21T16:44:12.109806Z",
          "iopub.status.idle": "2024-04-21T16:44:12.117623Z",
          "shell.execute_reply": "2024-04-21T16:44:12.116772Z"
        },
        "id": "YdvcMtx2ZSG_",
        "papermill": {
          "duration": 0.028206,
          "end_time": "2024-04-21T16:44:12.119455",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.091249",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "YdvcMtx2ZSG_"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.157067Z",
          "iopub.status.busy": "2024-04-21T16:44:12.156605Z",
          "iopub.status.idle": "2024-04-21T16:44:12.162773Z",
          "shell.execute_reply": "2024-04-21T16:44:12.161988Z"
        },
        "id": "90hjyHc_ZSG_",
        "papermill": {
          "duration": 0.026955,
          "end_time": "2024-04-21T16:44:12.164682",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.137727",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "90hjyHc_ZSG_"
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "\n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.201993Z",
          "iopub.status.busy": "2024-04-21T16:44:12.201745Z",
          "iopub.status.idle": "2024-04-21T16:44:12.211952Z",
          "shell.execute_reply": "2024-04-21T16:44:12.211113Z"
        },
        "id": "gNoNIQryZSHA",
        "papermill": {
          "duration": 0.031625,
          "end_time": "2024-04-21T16:44:12.213956",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.182331",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "gNoNIQryZSHA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a custom function to preprocess dataset according to our needs"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.294210Z",
          "iopub.status.busy": "2024-04-21T16:44:12.293864Z",
          "iopub.status.idle": "2024-04-21T16:44:12.297989Z",
          "shell.execute_reply": "2024-04-21T16:44:12.297201Z"
        },
        "id": "eqMAwtpHZSHA",
        "papermill": {
          "duration": 0.068037,
          "end_time": "2024-04-21T16:44:12.299880",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.231843",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "eqMAwtpHZSHA"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(g1):\n",
        "    # Apply the function to each row and save the result in a new column\n",
        "    g1['labels'] = g1.apply(lambda row: extract_words(row), axis=1)\n",
        "    #g1.drop('tags', axis=1, inplace=True)\n",
        "\n",
        "    # Apply functions to DataFrame\n",
        "    g1['text ids'] = g1['text'].apply(generate_text_ids)\n",
        "    g1['label ids'] = g1.apply(lambda row: generate_label_ids(row['labels'], row['text ids'], entityset), axis=1)\n",
        "\n",
        "    # Create a new column 'sentence' by joining the words in 'text_ids' with a space\n",
        "    g1['sentence'] = g1['text ids'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Create a new column 'word_labels' by converting the lists in 'text_ids' to comma-separated strings\n",
        "    g1['word_labels'] = g1['label ids'].apply(lambda x: ','.join(x))\n",
        "\n",
        "    g1.drop([\"text\", \"labels\", \"text ids\", \"label ids\", \"tags\"], axis=1, inplace=True)\n",
        "\n",
        "    data = g1\n",
        "\n",
        "    train_size = 0.8\n",
        "    train_dataset = data.sample(frac=train_size, random_state=200)\n",
        "    test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    # Create keep_dataset by randomly selecting 100 samples from train_dataset\n",
        "    keep_dataset = train_dataset.sample(n=100, random_state=42)\n",
        "    keep_dataset = keep_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(data.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "    print(\"KEEP Dataset: {}\".format(keep_dataset.shape))\n",
        "\n",
        "    training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "    testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
        "    keep_set = dataset(keep_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                    'shuffle': True,\n",
        "                    'num_workers': 0\n",
        "                    }\n",
        "\n",
        "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                   'shuffle': True,\n",
        "                   'num_workers': 0\n",
        "                   }\n",
        "\n",
        "    keep_params = {'batch_size': KEEP_BATCH_SIZE,\n",
        "                   'shuffle': True,\n",
        "                   'num_workers': 0\n",
        "                   }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    testing_loader = DataLoader(testing_set, **test_params)\n",
        "    keep_loader = DataLoader(keep_set, **keep_params)\n",
        "\n",
        "    return training_set, testing_set, keep_set, training_loader, testing_loader, keep_loader\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.337539Z",
          "iopub.status.busy": "2024-04-21T16:44:12.336971Z",
          "iopub.status.idle": "2024-04-21T16:44:12.349325Z",
          "shell.execute_reply": "2024-04-21T16:44:12.348525Z"
        },
        "id": "2r6884MLZSHA",
        "papermill": {
          "duration": 0.033022,
          "end_time": "2024-04-21T16:44:12.351178",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.318156",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "2r6884MLZSHA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Model**"
      ],
      "metadata": {
        "id": "3ZMSwixEZSHA",
        "papermill": {
          "duration": 0.017591,
          "end_time": "2024-04-21T16:44:12.386603",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.369012",
          "status": "completed"
        },
        "tags": []
      },
      "id": "3ZMSwixEZSHA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading bert-base, A 110M parameter model, <500MB\n",
        "# Everything that we achieve with this approach is good because this is the smallest popular transformer model\n",
        "\n",
        "from torch import cuda\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:12.423230Z",
          "iopub.status.busy": "2024-04-21T16:44:12.422938Z",
          "iopub.status.idle": "2024-04-21T16:44:19.660996Z",
          "shell.execute_reply": "2024-04-21T16:44:19.660014Z"
        },
        "id": "6_bi-SEmZSHB",
        "outputId": "257f554e-814e-44fe-b800-b1d23aeb1b0f",
        "papermill": {
          "duration": 7.258795,
          "end_time": "2024-04-21T16:44:19.663167",
          "exception": false,
          "start_time": "2024-04-21T16:44:12.404372",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "8ba799e9c9a94ea08a307ca0b5c2bd90",
            "9700672de60b495e8eebca2b95c4078c",
            "37a89712f6174cc2b8811f2d2cf95d0d",
            "d932bd66ef434bd7aed7f79463961f29",
            "7f1cfc114f17431a8ead115a79637663"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ba799e9c9a94ea08a307ca0b5c2bd90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9700672de60b495e8eebca2b95c4078c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37a89712f6174cc2b8811f2d2cf95d0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d932bd66ef434bd7aed7f79463961f29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f1cfc114f17431a8ead115a79637663",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=31, bias=True)\n",
              ")"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "6_bi-SEmZSHB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Train and Validation Functions**"
      ],
      "metadata": {
        "id": "TG3Xi9wXZSHF",
        "papermill": {
          "duration": 0.019933,
          "end_time": "2024-04-21T16:44:19.703865",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.683932",
          "status": "completed"
        },
        "tags": []
      },
      "id": "TG3Xi9wXZSHF"
    },
    {
      "cell_type": "code",
      "source": [
        "# # A custom training loop\n",
        "\n",
        "def train(epoch, training_loader, optimizer):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:19.748262Z",
          "iopub.status.busy": "2024-04-21T16:44:19.747399Z",
          "iopub.status.idle": "2024-04-21T16:44:19.759089Z",
          "shell.execute_reply": "2024-04-21T16:44:19.758353Z"
        },
        "id": "UyYcz2E1ZSHF",
        "papermill": {
          "duration": 0.036809,
          "end_time": "2024-04-21T16:44:19.761069",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.724260",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "UyYcz2E1ZSHF"
    },
    {
      "cell_type": "code",
      "source": [
        "# A custom validation loop\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:19.804525Z",
          "iopub.status.busy": "2024-04-21T16:44:19.804136Z",
          "iopub.status.idle": "2024-04-21T16:44:19.817138Z",
          "shell.execute_reply": "2024-04-21T16:44:19.816232Z"
        },
        "id": "4y9prD7HZSHG",
        "papermill": {
          "duration": 0.037226,
          "end_time": "2024-04-21T16:44:19.819105",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.781879",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "4y9prD7HZSHG"
    },
    {
      "cell_type": "code",
      "source": [
        "# A custom function which can do the training just by taking as input the train set\n",
        "# We do have a custom training function above but to call it, We need to go through all these steps\n",
        "# This function and the one below is for easy access as we will do a lot of training and eval\n",
        "\n",
        "def train_ner(training_set, training_loader):\n",
        "    ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "    mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "    targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "\n",
        "    ids = ids.to(device)\n",
        "    mask = mask.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "    initial_loss = outputs[0]\n",
        "    initial_loss\n",
        "\n",
        "    tr_logits = outputs[1]\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "        train(epoch, training_loader, optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:19.860333Z",
          "iopub.status.busy": "2024-04-21T16:44:19.860029Z",
          "iopub.status.idle": "2024-04-21T16:44:19.867462Z",
          "shell.execute_reply": "2024-04-21T16:44:19.866514Z"
        },
        "id": "eZ86rqYPZSHH",
        "papermill": {
          "duration": 0.03042,
          "end_time": "2024-04-21T16:44:19.869374",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.838954",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "eZ86rqYPZSHH"
    },
    {
      "cell_type": "code",
      "source": [
        "# A custom function which can do the validation just by taking as input the test set\n",
        "\n",
        "def val_ner(testing_loader):\n",
        "    labels, predictions = valid(model, testing_loader)\n",
        "    print(classification_report([labels], [predictions]))\n",
        "    return(classification_report([labels], [predictions],output_dict=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:19.911434Z",
          "iopub.status.busy": "2024-04-21T16:44:19.911120Z",
          "iopub.status.idle": "2024-04-21T16:44:19.915799Z",
          "shell.execute_reply": "2024-04-21T16:44:19.914946Z"
        },
        "id": "DYrQKzRrZSHI",
        "papermill": {
          "duration": 0.027781,
          "end_time": "2024-04-21T16:44:19.917752",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.889971",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "DYrQKzRrZSHI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training on G1 for Task T1**"
      ],
      "metadata": {
        "id": "5h1bAzOSZSHI",
        "papermill": {
          "duration": 0.020255,
          "end_time": "2024-04-21T16:44:19.957962",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.937707",
          "status": "completed"
        },
        "tags": []
      },
      "id": "5h1bAzOSZSHI"
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Args\n",
        "\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 16\n",
        "KEEP_BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ],
      "metadata": {
        "id": "AxY-r3PKrLpP"
      },
      "id": "AxY-r3PKrLpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting preprocessed dataset for task T1\n",
        "\n",
        "training_set_g1, testing_set_g1, keep_set_g1, training_loader_g1, testing_loader_g1, keep_loader_g1 = preprocess_dataset(g1)\n",
        "\n",
        "# Training\n",
        "\n",
        "train_ner(training_set_g1, training_loader_g1)\n",
        "\n",
        "# Validation\n",
        "\n",
        "cls_rep=val_ner(testing_loader_g1)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:44:20.011612Z",
          "iopub.status.busy": "2024-04-21T16:44:20.010555Z",
          "iopub.status.idle": "2024-04-21T16:55:40.687161Z",
          "shell.execute_reply": "2024-04-21T16:55:40.686348Z"
        },
        "id": "F-jDO_egZSHI",
        "outputId": "12a5bb3e-a983-4369-ecde-a3d1e241be23",
        "papermill": {
          "duration": 680.709637,
          "end_time": "2024-04-21T16:55:40.689516",
          "exception": false,
          "start_time": "2024-04-21T16:44:19.979879",
          "status": "completed"
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "FULL Dataset: (7356, 2)\n\nTRAIN Dataset: (5885, 2)\n\nTEST Dataset: (1471, 2)\n\nKEEP Dataset: (100, 2)\n\nTraining epoch: 1\n\nTraining loss per 100 training steps: 3.6199393272399902\n\nTraining loss per 100 training steps: 0.6424711991359692\n\nTraining loss epoch: 0.4332884680074842\n\nTraining accuracy epoch: 0.6491089502333726\n\nTraining epoch: 2\n\nTraining loss per 100 training steps: 0.17029117047786713\n\nTraining loss per 100 training steps: 0.13962984630967132\n\nTraining loss epoch: 0.12812983094836058\n\nTraining accuracy epoch: 0.7544144427047156\n\nTraining epoch: 3\n\nTraining loss per 100 training steps: 0.11376184970140457\n\nTraining loss per 100 training steps: 0.09827355738028441\n\nTraining loss epoch: 0.09380341430559107\n\nTraining accuracy epoch: 0.8188104728831705\n\nTraining epoch: 4\n\nTraining loss per 100 training steps: 0.11574897170066833\n\nTraining loss per 100 training steps: 0.07571210859730693\n\nTraining loss epoch: 0.07321728561478465\n\nTraining accuracy epoch: 0.8601670845844663\n\nTraining epoch: 5\n\nTraining loss per 100 training steps: 0.060596853494644165\n\nTraining loss per 100 training steps: 0.06289438316875165\n\nTraining loss epoch: 0.061129467845287014\n\nTraining accuracy epoch: 0.8840827711882696\n\nTraining epoch: 6\n\nTraining loss per 100 training steps: 0.06145770102739334\n\nTraining loss per 100 training steps: 0.05268446617934963\n\nTraining loss epoch: 0.051772705542490534\n\nTraining accuracy epoch: 0.9011145615403872\n\nTraining epoch: 7\n\nTraining loss per 100 training steps: 0.05013318732380867\n\nTraining loss per 100 training steps: 0.043377499732345635\n\nTraining loss epoch: 0.04408551713087312\n\nTraining accuracy epoch: 0.9157046831124983\n\nTraining epoch: 8\n\nTraining loss per 100 training steps: 0.024620935320854187\n\nTraining loss per 100 training steps: 0.037275347286964404\n\nTraining loss epoch: 0.03774328391148668\n\nTraining accuracy epoch: 0.9276769688996359\n\nTraining epoch: 9\n\nTraining loss per 100 training steps: 0.02218967117369175\n\nTraining loss per 100 training steps: 0.03228065895267052\n\nTraining loss epoch: 0.03265655808069784\n\nTraining accuracy epoch: 0.938411904119227\n\nTraining epoch: 10\n\nTraining loss per 100 training steps: 0.014179907739162445\n\nTraining loss per 100 training steps: 0.028888664005490224\n\nTraining loss epoch: 0.02829879803744995\n\nTraining accuracy epoch: 0.9473596874943947\n\nValidation loss per 100 evaluation steps: 0.13198232650756836\n\nValidation Loss: 0.06158899873211656\n\nValidation Accuracy: 0.8961880420452072\n\n                 precision    recall  f1-score   support\n\n\n\n   allergy_name       0.85      0.65      0.74       231\n\n         cancer       0.75      0.70      0.73      1032\n\nchronic_disease       0.78      0.78      0.78      2154\n\n      treatment       0.75      0.81      0.78      3014\n\n\n\n      micro avg       0.76      0.78      0.77      6431\n\n      macro avg       0.78      0.74      0.76      6431\n\n   weighted avg       0.76      0.78      0.77      6431\n\n\n"
        }
      ],
      "id": "F-jDO_egZSHI"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just for storing f1-score\n",
        "\n",
        "clsrep_t123comb = pd.DataFrame(cls_rep).transpose()\n",
        "# Convert dictionary to DataFrame\n",
        "cls_rep_df = pd.DataFrame(cls_rep)\n",
        "# Filter out rows containing \"micro avg\" and \"macro avg\"\n",
        "filtered_df = cls_rep_df[~cls_rep_df.index.isin(['micro avg', 'macro avg'])]\n",
        "# Extract the F1-score column\n",
        "f1_scores = filtered_df.loc['f1-score']\n",
        "# Optionally, convert the f1_scores to numeric if it's in string format\n",
        "f1_scores_t1 = pd.to_numeric(f1_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:55:40.734606Z",
          "iopub.status.busy": "2024-04-21T16:55:40.734076Z",
          "iopub.status.idle": "2024-04-21T16:55:40.743527Z",
          "shell.execute_reply": "2024-04-21T16:55:40.742759Z"
        },
        "papermill": {
          "duration": 0.033777,
          "end_time": "2024-04-21T16:55:40.745425",
          "exception": false,
          "start_time": "2024-04-21T16:55:40.711648",
          "status": "completed"
        },
        "tags": [],
        "id": "pgG9VuEnivHr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pgG9VuEnivHr"
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"KaggleMasterX/BERT_Episode1\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:55:40.789739Z",
          "iopub.status.busy": "2024-04-21T16:55:40.789041Z",
          "iopub.status.idle": "2024-04-21T16:56:01.036783Z",
          "shell.execute_reply": "2024-04-21T16:56:01.035867Z"
        },
        "papermill": {
          "duration": 20.271938,
          "end_time": "2024-04-21T16:56:01.038784",
          "exception": false,
          "start_time": "2024-04-21T16:55:40.766846",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cf0e3dc790bf45bbb3944c92c1af6c46"
          ]
        },
        "id": "95IU8_9CivHr",
        "outputId": "fac307cb-3aff-401a-e2ae-a62c102b0755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf0e3dc790bf45bbb3944c92c1af6c46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/KaggleMasterX/BERT_Episode1/commit/9588c30d3287b1a2b7d4be1ab6866938308b5779', commit_message='Upload BertForTokenClassification', commit_description='', oid='9588c30d3287b1a2b7d4be1ab6866938308b5779', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "95IU8_9CivHr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Functions for EWC based Training**"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.021711,
          "end_time": "2024-04-21T16:56:01.082905",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.061194",
          "status": "completed"
        },
        "tags": [],
        "id": "WmDzug74ivHr"
      },
      "id": "WmDzug74ivHr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom function for calculation of Fisher Information\n",
        "\n",
        "def compute_fisher_information(training_loader, model, device):\n",
        "    fisher_information = {}\n",
        "    model.train()\n",
        "\n",
        "    for batch in training_loader:\n",
        "        ids = batch['ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['mask'].to(device, dtype=torch.long)\n",
        "        targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Compute squared gradients and accumulate\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if name not in fisher_information:\n",
        "                    fisher_information[name] = (param.grad.detach() ** 2).clone()\n",
        "                else:\n",
        "                    fisher_information[name] += (param.grad.detach() ** 2).clone()\n",
        "\n",
        "    return fisher_information\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:56:01.129103Z",
          "iopub.status.busy": "2024-04-21T16:56:01.128576Z",
          "iopub.status.idle": "2024-04-21T16:56:01.137368Z",
          "shell.execute_reply": "2024-04-21T16:56:01.136428Z"
        },
        "papermill": {
          "duration": 0.034722,
          "end_time": "2024-04-21T16:56:01.139362",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.104640",
          "status": "completed"
        },
        "tags": [],
        "id": "F9QDTkxNivHs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "F9QDTkxNivHs"
    },
    {
      "cell_type": "code",
      "source": [
        "# A custom function which calculates scale factor taking as input the fisher information of the parameters\n",
        "\n",
        "def compute_ewc_lambda(fisher_information, scale_factor=1.0):\n",
        "    \"\"\"\n",
        "    Compute the value of ewc_lambda (λ) based on the magnitude of Fisher Information.\n",
        "\n",
        "    Args:\n",
        "    - fisher_information (dict): Dictionary containing Fisher Information for each parameter.\n",
        "    - scale_factor (float): Scaling factor to adjust the magnitude of λ. Default is 1.0.\n",
        "\n",
        "    Returns:\n",
        "    - ewc_lambda (float): Value of ewc_lambda (λ) that is inversely proportional to the magnitude of Fisher Information.\n",
        "    \"\"\"\n",
        "    if not fisher_information:\n",
        "        return 0.0\n",
        "\n",
        "    total_fisher = sum(torch.sum(value) for value in fisher_information.values())\n",
        "    if total_fisher == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ewc_lambda = scale_factor / total_fisher\n",
        "    return ewc_lambda\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:56:01.188330Z",
          "iopub.status.busy": "2024-04-21T16:56:01.187750Z",
          "iopub.status.idle": "2024-04-21T16:56:01.193979Z",
          "shell.execute_reply": "2024-04-21T16:56:01.193072Z"
        },
        "papermill": {
          "duration": 0.032374,
          "end_time": "2024-04-21T16:56:01.195887",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.163513",
          "status": "completed"
        },
        "tags": [],
        "id": "ECj26cT3ivHs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ECj26cT3ivHs"
    },
    {
      "cell_type": "code",
      "source": [
        "# In elastic weight consolidation, We introduce a regularization term which constraints the parameter updates\n",
        "# This constraint makes sure that Model doesn't forget previous knowledge\n",
        "# I have made a custom training function for training the model using fisher information based constraints\n",
        "\n",
        "def train_with_ewc(training_set, training_loader, ewc_lambda, fisher_information, device, model, optimizer):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    param_prior = {}  # Store the parameters from the previous task\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        param_prior[name] = param.detach().clone().cpu()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "\n",
        "        for idx, batch in enumerate(training_loader):\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss = outputs.loss\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples += targets.size(0)\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = outputs.logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            tr_preds.extend(predictions)\n",
        "            tr_labels.extend(targets)\n",
        "\n",
        "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "            # Compute EWC loss\n",
        "            ewc_loss = 0\n",
        "            if fisher_information:\n",
        "                for name, param in model.named_parameters():\n",
        "                    if name in fisher_information:\n",
        "                        importance = fisher_information[name]\n",
        "                        param_prior[name] = param_prior[name].to(device)  # Move param_prior to the same device as param\n",
        "                        ewc_loss += (importance * (param - param_prior[name]) ** 2).sum()\n",
        "\n",
        "            total_loss = loss + ewc_lambda * ewc_loss if ewc_loss != 0 else loss\n",
        "\n",
        "            # gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "            )\n",
        "\n",
        "            # backward pass\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss = tr_loss / nb_tr_steps\n",
        "        tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "        print(f\"Training loss epoch: {epoch_loss}\")\n",
        "        print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:56:01.241020Z",
          "iopub.status.busy": "2024-04-21T16:56:01.240704Z",
          "iopub.status.idle": "2024-04-21T16:56:01.255507Z",
          "shell.execute_reply": "2024-04-21T16:56:01.254611Z"
        },
        "papermill": {
          "duration": 0.039658,
          "end_time": "2024-04-21T16:56:01.257428",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.217770",
          "status": "completed"
        },
        "tags": [],
        "id": "MFLtrkxUivHs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MFLtrkxUivHs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK 2**"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.022912,
          "end_time": "2024-04-21T16:56:01.303053",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.280141",
          "status": "completed"
        },
        "tags": [],
        "id": "2Pi_L0yhivHs"
      },
      "id": "2Pi_L0yhivHs"
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_information = compute_fisher_information(training_loader_g1, model, device)\n",
        "\n",
        "# Computing the Fisher Information for parameters of the Neural Network\n",
        "# Represents the importance of paramters"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:56:01.399453Z",
          "iopub.status.busy": "2024-04-21T16:56:01.398930Z",
          "iopub.status.idle": "2024-04-21T16:57:05.243607Z",
          "shell.execute_reply": "2024-04-21T16:57:05.242756Z"
        },
        "papermill": {
          "duration": 63.869851,
          "end_time": "2024-04-21T16:57:05.245975",
          "exception": false,
          "start_time": "2024-04-21T16:56:01.376124",
          "status": "completed"
        },
        "tags": [],
        "id": "PYu0HWDwivHx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PYu0HWDwivHx"
    },
    {
      "cell_type": "code",
      "source": [
        "ewclambda=compute_ewc_lambda(fisher_information)\n",
        "\n",
        "# EWC Lambda is a scaling term used in EWC method, It should be inversely proportional to the general trend of fisher infomration\n",
        "# We have made a function above which calculates a value that follows the above description"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:57:05.350683Z",
          "iopub.status.busy": "2024-04-21T16:57:05.349961Z",
          "iopub.status.idle": "2024-04-21T16:57:05.414543Z",
          "shell.execute_reply": "2024-04-21T16:57:05.413716Z"
        },
        "papermill": {
          "duration": 0.091535,
          "end_time": "2024-04-21T16:57:05.416820",
          "exception": false,
          "start_time": "2024-04-21T16:57:05.325285",
          "status": "completed"
        },
        "tags": [],
        "id": "eu9XQoEbivHx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eu9XQoEbivHx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting preprocessed dataset for Task T2\n",
        "\n",
        "training_set_g2, testing_set_g2, keep_set_g2, training_loader_g2, testing_loader_g2, keep_loader_g2 = preprocess_dataset(g2)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:57:05.518997Z",
          "iopub.status.busy": "2024-04-21T16:57:05.518654Z",
          "iopub.status.idle": "2024-04-21T16:57:05.990320Z",
          "shell.execute_reply": "2024-04-21T16:57:05.989025Z"
        },
        "papermill": {
          "duration": 0.498023,
          "end_time": "2024-04-21T16:57:05.992366",
          "exception": false,
          "start_time": "2024-04-21T16:57:05.494343",
          "status": "completed"
        },
        "tags": [],
        "id": "ma0x_IEaivHx",
        "outputId": "e5268ea4-3e57-41d9-e3aa-b570f373419c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "FULL Dataset: (6455, 2)\n\nTRAIN Dataset: (5164, 2)\n\nTEST Dataset: (1291, 2)\n\nKEEP Dataset: (100, 2)\n"
        }
      ],
      "id": "ma0x_IEaivHx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our model on Task T1 while following a regularization powered by importance of weights of the model.\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_with_ewc(training_set_g2, training_loader_g2, ewclambda, fisher_information, device, model, optimizer)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T16:57:06.039717Z",
          "iopub.status.busy": "2024-04-21T16:57:06.039043Z",
          "iopub.status.idle": "2024-04-21T17:07:53.166753Z",
          "shell.execute_reply": "2024-04-21T17:07:53.165901Z"
        },
        "papermill": {
          "duration": 647.153697,
          "end_time": "2024-04-21T17:07:53.169596",
          "exception": false,
          "start_time": "2024-04-21T16:57:06.015899",
          "status": "completed"
        },
        "tags": [],
        "id": "7VvvAlQ-ivHx",
        "outputId": "1deb0aa3-516b-4640-e44a-d27bea73717e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training epoch: 1\n\nTraining loss epoch: 0.06507821040756909\n\nTraining accuracy epoch: 0.8692004453909362\n\nTraining epoch: 2\n\nTraining loss epoch: 0.05935637729909317\n\nTraining accuracy epoch: 0.4469961337790146\n\nTraining epoch: 3\n\nTraining loss epoch: 0.055069734120105275\n\nTraining accuracy epoch: 0.3020116179149848\n\nTraining epoch: 4\n\nTraining loss epoch: 0.05155102501013949\n\nTraining accuracy epoch: 0.22914892512443227\n\nTraining epoch: 5\n\nTraining loss epoch: 0.048381548347296535\n\nTraining accuracy epoch: 0.18567458734059983\n\nTraining epoch: 6\n\nTraining loss epoch: 0.045491128546710856\n\nTraining accuracy epoch: 0.15617945609673511\n\nTraining epoch: 7\n\nTraining loss epoch: 0.04285718960362294\n\nTraining accuracy epoch: 0.1348801282247596\n\nTraining epoch: 8\n\nTraining loss epoch: 0.040503083447994734\n\nTraining accuracy epoch: 0.11901899820698149\n\nTraining epoch: 9\n\nTraining loss epoch: 0.038322991471793834\n\nTraining accuracy epoch: 0.10649774022840826\n\nTraining epoch: 10\n\nTraining loss epoch: 0.03630025508428383\n\nTraining accuracy epoch: 0.0963023969811566\n"
        }
      ],
      "id": "7VvvAlQ-ivHx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model on 100 kept samples from Task T1 [ Our model works well even without this, But this is one approach to continual learning ]\n",
        "\n",
        "train_ner(keep_set_g1, keep_loader_g1)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:07:53.219546Z",
          "iopub.status.busy": "2024-04-21T17:07:53.218600Z",
          "iopub.status.idle": "2024-04-21T17:08:05.819826Z",
          "shell.execute_reply": "2024-04-21T17:08:05.818573Z"
        },
        "papermill": {
          "duration": 12.62809,
          "end_time": "2024-04-21T17:08:05.822008",
          "exception": false,
          "start_time": "2024-04-21T17:07:53.193918",
          "status": "completed"
        },
        "tags": [],
        "id": "A-ozbsjoivHy",
        "outputId": "1b3a0cdb-4ead-4a45-e4c4-8a764ccbb5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training epoch: 1\n\nTraining loss per 100 training steps: 0.026385173201560974\n\nTraining loss epoch: 0.031522838664906364\n\nTraining accuracy epoch: 0.9284241321444108\n\nTraining epoch: 2\n\nTraining loss per 100 training steps: 0.0281931571662426\n\nTraining loss epoch: 0.025137238004910096\n\nTraining accuracy epoch: 0.9451320178952501\n\nTraining epoch: 3\n\nTraining loss per 100 training steps: 0.01892845332622528\n\nTraining loss epoch: 0.018432808374719962\n\nTraining accuracy epoch: 0.9664601252006328\n\nTraining epoch: 4\n\nTraining loss per 100 training steps: 0.017087770625948906\n\nTraining loss epoch: 0.012774807401001453\n\nTraining accuracy epoch: 0.9760734224545874\n\nTraining epoch: 5\n\nTraining loss per 100 training steps: 0.012372449971735477\n\nTraining loss epoch: 0.009911395409809691\n\nTraining accuracy epoch: 0.98104411330375\n\nTraining epoch: 6\n\nTraining loss per 100 training steps: 0.007793717551976442\n\nTraining loss epoch: 0.010178630373307638\n\nTraining accuracy epoch: 0.9742457700157594\n\nTraining epoch: 7\n\nTraining loss per 100 training steps: 0.0033181747421622276\n\nTraining loss epoch: 0.005972659165438797\n\nTraining accuracy epoch: 0.9894679434570699\n\nTraining epoch: 8\n\nTraining loss per 100 training steps: 0.0020396444015204906\n\nTraining loss epoch: 0.00441994220351002\n\nTraining accuracy epoch: 0.99224747179653\n\nTraining epoch: 9\n\nTraining loss per 100 training steps: 0.009541209787130356\n\nTraining loss epoch: 0.006580137531273067\n\nTraining accuracy epoch: 0.9891946864543743\n\nTraining epoch: 10\n\nTraining loss per 100 training steps: 0.003842028556391597\n\nTraining loss epoch: 0.0032744277601263355\n\nTraining accuracy epoch: 0.9962302231962868\n"
        }
      ],
      "id": "A-ozbsjoivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing combined data for Tasks T1, T2 (We'll use this for validation in the next cell)\n",
        "\n",
        "training_set_g1_g2, testing_set_g1_g2, keep_set_g1_g2, training_loader_g1_g2, testing_loader_g1_g2, keep_loader_g1_g2 = preprocess_dataset(g1_g2)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:08:05.872825Z",
          "iopub.status.busy": "2024-04-21T17:08:05.872486Z",
          "iopub.status.idle": "2024-04-21T17:08:07.267234Z",
          "shell.execute_reply": "2024-04-21T17:08:07.266138Z"
        },
        "papermill": {
          "duration": 1.422943,
          "end_time": "2024-04-21T17:08:07.269638",
          "exception": false,
          "start_time": "2024-04-21T17:08:05.846695",
          "status": "completed"
        },
        "tags": [],
        "id": "MMFLZICZivHy",
        "outputId": "d43b2d05-4597-4d39-e5f9-4eff2e293788"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "FULL Dataset: (20088, 2)\n\nTRAIN Dataset: (16070, 2)\n\nTEST Dataset: (4018, 2)\n\nKEEP Dataset: (100, 2)\n"
        }
      ],
      "id": "MMFLZICZivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating the model on Tasks T1 and T2 after our continual learning approach\n",
        "\n",
        "cls_rep=val_ner(testing_loader_g1_g2)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:08:07.322910Z",
          "iopub.status.busy": "2024-04-21T17:08:07.322099Z",
          "iopub.status.idle": "2024-04-21T17:08:37.708539Z",
          "shell.execute_reply": "2024-04-21T17:08:37.707690Z"
        },
        "papermill": {
          "duration": 30.41415,
          "end_time": "2024-04-21T17:08:37.711029",
          "exception": false,
          "start_time": "2024-04-21T17:08:07.296879",
          "status": "completed"
        },
        "tags": [],
        "id": "TMiw1VPHivHy",
        "outputId": "74a014fb-0edd-4051-a306-f729429050f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Validation loss per 100 evaluation steps: 0.03432496264576912\n\nValidation loss per 100 evaluation steps: 0.059190237286067246\n\nValidation loss per 100 evaluation steps: 0.058233328696470056\n\nValidation Loss: 0.05790267709184379\n\nValidation Accuracy: 0.916592252571287\n\n                 precision    recall  f1-score   support\n\n\n\n   allergy_name       0.83      0.82      0.82       711\n\n         cancer       0.77      0.82      0.79      2712\n\nchronic_disease       0.76      0.85      0.81      6781\n\n      treatment       0.80      0.88      0.84      8836\n\n\n\n      micro avg       0.78      0.86      0.82     19040\n\n      macro avg       0.79      0.84      0.82     19040\n\n   weighted avg       0.78      0.86      0.82     19040\n\n\n"
        }
      ],
      "id": "TMiw1VPHivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just to store f1-score\n",
        "\n",
        "clsrep_t123comb = pd.DataFrame(cls_rep).transpose()\n",
        "# Convert dictionary to DataFrame\n",
        "cls_rep_df = pd.DataFrame(cls_rep)\n",
        "# Filter out rows containing \"micro avg\" and \"macro avg\"\n",
        "filtered_df = cls_rep_df[~cls_rep_df.index.isin(['micro avg', 'macro avg'])]\n",
        "# Extract the F1-score column\n",
        "f1_scores = filtered_df.loc['f1-score']\n",
        "# Optionally, convert the f1_scores to numeric if it's in string format\n",
        "f1_scores_t1t2 = pd.to_numeric(f1_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:08:37.764852Z",
          "iopub.status.busy": "2024-04-21T17:08:37.764499Z",
          "iopub.status.idle": "2024-04-21T17:08:37.772761Z",
          "shell.execute_reply": "2024-04-21T17:08:37.771655Z"
        },
        "papermill": {
          "duration": 0.036669,
          "end_time": "2024-04-21T17:08:37.774794",
          "exception": false,
          "start_time": "2024-04-21T17:08:37.738125",
          "status": "completed"
        },
        "tags": [],
        "id": "dBmFGIRpivHy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dBmFGIRpivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"KaggleMasterX/BERT_Episode2\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:08:37.825829Z",
          "iopub.status.busy": "2024-04-21T17:08:37.825237Z",
          "iopub.status.idle": "2024-04-21T17:08:52.636171Z",
          "shell.execute_reply": "2024-04-21T17:08:52.635080Z"
        },
        "papermill": {
          "duration": 14.838773,
          "end_time": "2024-04-21T17:08:52.638218",
          "exception": false,
          "start_time": "2024-04-21T17:08:37.799445",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "18b61b3974b7424c85b21d9ffc824ae1"
          ]
        },
        "id": "ckbHvg3OivHy",
        "outputId": "b57cad1e-94c8-4734-90e4-ce4cd21c6368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18b61b3974b7424c85b21d9ffc824ae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/KaggleMasterX/BERT_Episode2/commit/78727807647b2c84fe69bd1ad644dd5742e67393', commit_message='Upload BertForTokenClassification', commit_description='', oid='78727807647b2c84fe69bd1ad644dd5742e67393', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "ckbHvg3OivHy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK 3**"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.02497,
          "end_time": "2024-04-21T17:08:52.689477",
          "exception": false,
          "start_time": "2024-04-21T17:08:52.664507",
          "status": "completed"
        },
        "tags": [],
        "id": "qsvRYGwOivHy"
      },
      "id": "qsvRYGwOivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing Fisher Information of Model paramters after 2 episodes, and computing a Lambda for the same\n",
        "\n",
        "fisher_information = compute_fisher_information(training_loader_g2, model, device)\n",
        "\n",
        "ewclambda=compute_ewc_lambda(fisher_information)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:08:52.741946Z",
          "iopub.status.busy": "2024-04-21T17:08:52.741065Z",
          "iopub.status.idle": "2024-04-21T17:09:48.922683Z",
          "shell.execute_reply": "2024-04-21T17:09:48.921688Z"
        },
        "papermill": {
          "duration": 56.210111,
          "end_time": "2024-04-21T17:09:48.924982",
          "exception": false,
          "start_time": "2024-04-21T17:08:52.714871",
          "status": "completed"
        },
        "tags": [],
        "id": "6JyViMUUivHy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6JyViMUUivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the data for Task T3\n",
        "\n",
        "training_set_g3, testing_set_g3, keep_set_g3, training_loader_g3, testing_loader_g3, keep_loader_g3 = preprocess_dataset(g3)\n",
        "\n",
        "# Training the current model on Task T3 now\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_with_ewc(training_set_g3, training_loader_g3, ewclambda, fisher_information, device, model, optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:09:48.977211Z",
          "iopub.status.busy": "2024-04-21T17:09:48.976496Z",
          "iopub.status.idle": "2024-04-21T17:20:18.473698Z",
          "shell.execute_reply": "2024-04-21T17:20:18.472836Z"
        },
        "papermill": {
          "duration": 629.525417,
          "end_time": "2024-04-21T17:20:18.475926",
          "exception": false,
          "start_time": "2024-04-21T17:09:48.950509",
          "status": "completed"
        },
        "tags": [],
        "id": "-DYK-rNLivHy",
        "outputId": "d189530b-0fd9-4589-c300-764fdcfd5301"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "FULL Dataset: (6277, 2)\n\nTRAIN Dataset: (5022, 2)\n\nTEST Dataset: (1255, 2)\n\nKEEP Dataset: (100, 2)\n\nTraining epoch: 1\n\nTraining loss epoch: 0.06058806278238631\n\nTraining accuracy epoch: 0.8802105538927782\n\nTraining epoch: 2\n\nTraining loss epoch: 0.05494608188486973\n\nTraining accuracy epoch: 0.45209191590695186\n\nTraining epoch: 3\n\nTraining loss epoch: 0.050767574016392356\n\nTraining accuracy epoch: 0.30503720005111934\n\nTraining epoch: 4\n\nTraining loss epoch: 0.04730835868116871\n\nTraining accuracy epoch: 0.23144904444461725\n\nTraining epoch: 5\n\nTraining loss epoch: 0.04430681694014247\n\nTraining accuracy epoch: 0.18707669053554568\n\nTraining epoch: 6\n\nTraining loss epoch: 0.04159930321603476\n\nTraining accuracy epoch: 0.1572810706834929\n\nTraining epoch: 7\n\nTraining loss epoch: 0.039092102478698126\n\nTraining accuracy epoch: 0.1360374772881113\n\nTraining epoch: 8\n\nTraining loss epoch: 0.03677321876774418\n\nTraining accuracy epoch: 0.11996813423902722\n\nTraining epoch: 9\n\nTraining loss epoch: 0.03470813609021653\n\nTraining accuracy epoch: 0.10720743175352962\n\nTraining epoch: 10\n\nTraining loss epoch: 0.03276613108890879\n\nTraining accuracy epoch: 0.09706617169957893\n"
        }
      ],
      "id": "-DYK-rNLivHy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on just 100 preserved samples from T1 and T2 combined [ Our model performs well even without this but this is one approach to Continual Learning ]\n",
        "\n",
        "train_ner(keep_set_g1, keep_loader_g1)\n",
        "train_ner(keep_set_g2, keep_loader_g2)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:20:18.529164Z",
          "iopub.status.busy": "2024-04-21T17:20:18.528859Z",
          "iopub.status.idle": "2024-04-21T17:20:43.745461Z",
          "shell.execute_reply": "2024-04-21T17:20:43.744367Z"
        },
        "papermill": {
          "duration": 25.24554,
          "end_time": "2024-04-21T17:20:43.747566",
          "exception": false,
          "start_time": "2024-04-21T17:20:18.502026",
          "status": "completed"
        },
        "tags": [],
        "id": "KEn6CcitivHz",
        "outputId": "ef53fd69-00f8-44ce-9b9e-432ba05818fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training epoch: 1\n\nTraining loss per 100 training steps: 0.034248173236846924\n\nTraining loss epoch: 0.02340352162718773\n\nTraining accuracy epoch: 0.9518195127875027\n\nTraining epoch: 2\n\nTraining loss per 100 training steps: 0.008711077272891998\n\nTraining loss epoch: 0.010410700725125415\n\nTraining accuracy epoch: 0.975549383527377\n\nTraining epoch: 3\n\nTraining loss per 100 training steps: 0.00996107142418623\n\nTraining loss epoch: 0.008298155053385667\n\nTraining accuracy epoch: 0.9807477355571758\n\nTraining epoch: 4\n\nTraining loss per 100 training steps: 0.007673389744013548\n\nTraining loss epoch: 0.005076557630673051\n\nTraining accuracy epoch: 0.9918812912790145\n\nTraining epoch: 5\n\nTraining loss per 100 training steps: 0.003785453736782074\n\nTraining loss epoch: 0.003921936033293605\n\nTraining accuracy epoch: 0.9938028907036103\n\nTraining epoch: 6\n\nTraining loss per 100 training steps: 0.004346095956861973\n\nTraining loss epoch: 0.003415538530264582\n\nTraining accuracy epoch: 0.995489435301393\n\nTraining epoch: 7\n\nTraining loss per 100 training steps: 0.0014769629342481494\n\nTraining loss epoch: 0.0025611358328855465\n\nTraining accuracy epoch: 0.9974807172804948\n\nTraining epoch: 8\n\nTraining loss per 100 training steps: 0.0031388404313474894\n\nTraining loss epoch: 0.001935948376610343\n\nTraining accuracy epoch: 0.997495981121457\n\nTraining epoch: 9\n\nTraining loss per 100 training steps: 0.0016898479079827666\n\nTraining loss epoch: 0.001691219912442778\n\nTraining accuracy epoch: 0.9996685449121644\n\nTraining epoch: 10\n\nTraining loss per 100 training steps: 0.0013173045590519905\n\nTraining loss epoch: 0.0019592559380855945\n\nTraining accuracy epoch: 0.9990539262062441\n\nTraining epoch: 1\n\nTraining loss per 100 training steps: 0.02420436590909958\n\nTraining loss epoch: 0.028068438305386474\n\nTraining accuracy epoch: 0.9419211247114055\n\nTraining epoch: 2\n\nTraining loss per 100 training steps: 0.023056404665112495\n\nTraining loss epoch: 0.015666384183402573\n\nTraining accuracy epoch: 0.9661722315228977\n\nTraining epoch: 3\n\nTraining loss per 100 training steps: 0.0058473749086260796\n\nTraining loss epoch: 0.008341491355427675\n\nTraining accuracy epoch: 0.9793716849938646\n\nTraining epoch: 4\n\nTraining loss per 100 training steps: 0.012226647697389126\n\nTraining loss epoch: 0.007813157264276274\n\nTraining accuracy epoch: 0.9818871593707242\n\nTraining epoch: 5\n\nTraining loss per 100 training steps: 0.003731970675289631\n\nTraining loss epoch: 0.005357782655794706\n\nTraining accuracy epoch: 0.9919131277601831\n\nTraining epoch: 6\n\nTraining loss per 100 training steps: 0.006398973520845175\n\nTraining loss epoch: 0.006816624663770199\n\nTraining accuracy epoch: 0.9812232955984038\n\nTraining epoch: 7\n\nTraining loss per 100 training steps: 0.0074456422589719296\n\nTraining loss epoch: 0.004910354485868343\n\nTraining accuracy epoch: 0.9919349336651856\n\nTraining epoch: 8\n\nTraining loss per 100 training steps: 0.002195375505834818\n\nTraining loss epoch: 0.0030124194620709333\n\nTraining accuracy epoch: 0.9955334297041807\n\nTraining epoch: 9\n\nTraining loss per 100 training steps: 0.0011319337645545602\n\nTraining loss epoch: 0.0023632639786228538\n\nTraining accuracy epoch: 0.9958808447471732\n\nTraining epoch: 10\n\nTraining loss per 100 training steps: 0.002818731591105461\n\nTraining loss epoch: 0.0014918063783885113\n\nTraining accuracy epoch: 0.9992729916394039\n"
        }
      ],
      "id": "KEn6CcitivHz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the combined dataset for all tasks T1, T2 and T3\n",
        "\n",
        "training_set_g, testing_set_g, keep_set_g, training_loader_g, testing_loader_g, keep_loader_g = preprocess_dataset(all_g)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:20:43.804156Z",
          "iopub.status.busy": "2024-04-21T17:20:43.803840Z",
          "iopub.status.idle": "2024-04-21T17:20:45.219530Z",
          "shell.execute_reply": "2024-04-21T17:20:45.218541Z"
        },
        "papermill": {
          "duration": 1.446656,
          "end_time": "2024-04-21T17:20:45.222010",
          "exception": false,
          "start_time": "2024-04-21T17:20:43.775354",
          "status": "completed"
        },
        "tags": [],
        "id": "rpudErJVivHz",
        "outputId": "206d5889-fa2a-46b5-fdb1-e0ec7bb7020b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "FULL Dataset: (20088, 2)\n\nTRAIN Dataset: (16070, 2)\n\nTEST Dataset: (4018, 2)\n\nKEEP Dataset: (100, 2)\n"
        }
      ],
      "id": "rpudErJVivHz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating our continual learning based Model on all tasks T1, T2 and T3\n",
        "\n",
        "cls_rep=val_ner(testing_loader_g)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:20:45.281062Z",
          "iopub.status.busy": "2024-04-21T17:20:45.280725Z",
          "iopub.status.idle": "2024-04-21T17:21:15.439866Z",
          "shell.execute_reply": "2024-04-21T17:21:15.439070Z"
        },
        "papermill": {
          "duration": 30.191427,
          "end_time": "2024-04-21T17:21:15.442101",
          "exception": false,
          "start_time": "2024-04-21T17:20:45.250674",
          "status": "completed"
        },
        "tags": [],
        "id": "vKFOnLxDivHz",
        "outputId": "e6ceee7c-29d6-4ef5-9d03-14ba586d5feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Validation loss per 100 evaluation steps: 0.08108529448509216\n\nValidation loss per 100 evaluation steps: 0.04385929467216466\n\nValidation loss per 100 evaluation steps: 0.04478024677964233\n\nValidation Loss: 0.04432936635497581\n\nValidation Accuracy: 0.9366263113065599\n\n                 precision    recall  f1-score   support\n\n\n\n   allergy_name       0.90      0.90      0.90       711\n\n         cancer       0.78      0.89      0.83      2712\n\nchronic_disease       0.84      0.87      0.85      6781\n\n      treatment       0.87      0.90      0.88      8836\n\n\n\n      micro avg       0.85      0.89      0.87     19040\n\n      macro avg       0.85      0.89      0.87     19040\n\n   weighted avg       0.85      0.89      0.87     19040\n\n\n"
        }
      ],
      "id": "vKFOnLxDivHz"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just to fetch the f1-score\n",
        "\n",
        "clsrep_t123comb = pd.DataFrame(cls_rep).transpose()\n",
        "# Convert dictionary to DataFrame\n",
        "cls_rep_df = pd.DataFrame(cls_rep)\n",
        "# Filter out rows containing \"micro avg\" and \"macro avg\"\n",
        "filtered_df = cls_rep_df[~cls_rep_df.index.isin(['micro avg', 'macro avg'])]\n",
        "# Extract the F1-score column\n",
        "f1_scores = filtered_df.loc['f1-score']\n",
        "# Optionally, convert the f1_scores to numeric if it's in string format\n",
        "f1_scores_t1t2t3 = pd.to_numeric(f1_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:21:15.500642Z",
          "iopub.status.busy": "2024-04-21T17:21:15.500135Z",
          "iopub.status.idle": "2024-04-21T17:21:15.508206Z",
          "shell.execute_reply": "2024-04-21T17:21:15.507354Z"
        },
        "papermill": {
          "duration": 0.039094,
          "end_time": "2024-04-21T17:21:15.510103",
          "exception": false,
          "start_time": "2024-04-21T17:21:15.471009",
          "status": "completed"
        },
        "tags": [],
        "id": "XgoYSMj4ivHz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XgoYSMj4ivHz"
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"KaggleMasterX/BERT_Episode3\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:21:15.678662Z",
          "iopub.status.busy": "2024-04-21T17:21:15.678303Z",
          "iopub.status.idle": "2024-04-21T17:21:32.089154Z",
          "shell.execute_reply": "2024-04-21T17:21:32.088176Z"
        },
        "papermill": {
          "duration": 16.442156,
          "end_time": "2024-04-21T17:21:32.091142",
          "exception": false,
          "start_time": "2024-04-21T17:21:15.648986",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "d59e71a6525741d4ba5ea41bb70690e3"
          ]
        },
        "id": "1KcLDis3ivHz",
        "outputId": "3788afe0-bd41-49db-ab18-309090941910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d59e71a6525741d4ba5ea41bb70690e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 50,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/KaggleMasterX/BERT_Episode3/commit/8ebefb8835611b31105530a436501580d6fbc19f', commit_message='Upload BertForTokenClassification', commit_description='', oid='8ebefb8835611b31105530a436501580d6fbc19f', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "1KcLDis3ivHz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 4 (For Testing)**"
      ],
      "metadata": {
        "id": "yZTJMUVNivHz"
      },
      "id": "yZTJMUVNivHz"
    },
    {
      "cell_type": "code",
      "source": [
        "#g4=pd.read_excel(\"/kaggle/input/miimasa/G4.xlsx\")"
      ],
      "metadata": {
        "id": "b67qL80YivH0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b67qL80YivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"fisher_information = compute_fisher_information(training_loader_g3, model, device)\n",
        "\n",
        "ewclambda=compute_ewc_lambda(fisher_information)\n",
        "\n",
        "\n",
        "# Getting preprocessed dataset for a given dataframe\n",
        "\n",
        "training_set_g4, testing_set_g4, keep_set_g4, training_loader_g4, testing_loader_g4, keep_loader_g4 = preprocess_dataset(g4)\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Call the function\n",
        "train_with_ewc(training_set_g4, training_loader_g4, ewclambda, fisher_information, device, model, optimizer)\n",
        "\n",
        "\n",
        "# Validating the Updated Model on Tasks T1, T2, T3 after using those previous 100 examples\n",
        "\n",
        "train_ner(keep_set_g, keep_loader_g)\n",
        "val_ner(testing_loader_g)\"\"\"\""
      ],
      "metadata": {
        "id": "s7XvZdUzivH0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "s7XvZdUzivH0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please use a file G4.csv to run the above 2 cells and it will be a test of our continual learning approach"
      ],
      "metadata": {
        "id": "07G1_NsuivH0"
      },
      "id": "07G1_NsuivH0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Model with G1, G2, G3 Combined**"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.02822,
          "end_time": "2024-04-21T17:21:32.148472",
          "exception": false,
          "start_time": "2024-04-21T17:21:32.120252",
          "status": "completed"
        },
        "tags": [],
        "id": "nbxbQup2ivH0"
      },
      "id": "nbxbQup2ivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Model again to ensure it is pretrained model and not our fine-tuned model\n",
        "\n",
        "from torch import cuda\n",
        "\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:21:32.207814Z",
          "iopub.status.busy": "2024-04-21T17:21:32.207480Z",
          "iopub.status.idle": "2024-04-21T17:21:32.842227Z",
          "shell.execute_reply": "2024-04-21T17:21:32.841201Z"
        },
        "papermill": {
          "duration": 0.666914,
          "end_time": "2024-04-21T17:21:32.844184",
          "exception": false,
          "start_time": "2024-04-21T17:21:32.177270",
          "status": "completed"
        },
        "tags": [],
        "id": "785srDCVivH0",
        "outputId": "74c5baa7-ab0e-4dad-a70a-ef1e964e0628"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "execution_count": 51,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=31, bias=True)\n",
              ")"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "785srDCVivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fine-Tuning on Pre-Trained BERT-Base using all datasets together\n",
        "\n",
        "train_ner(training_set_g, training_loader_g)\n",
        "\n",
        "# Validation on the validation set produced out of all datasets collectively\n",
        "\n",
        "val_ner(testing_loader_g)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:21:32.903876Z",
          "iopub.status.busy": "2024-04-21T17:21:32.903578Z",
          "iopub.status.idle": "2024-04-21T17:52:29.604587Z",
          "shell.execute_reply": "2024-04-21T17:52:29.603490Z"
        },
        "papermill": {
          "duration": 1856.771159,
          "end_time": "2024-04-21T17:52:29.644814",
          "exception": false,
          "start_time": "2024-04-21T17:21:32.873655",
          "status": "completed"
        },
        "tags": [],
        "id": "E-5GDIiSivH0",
        "outputId": "87af00fd-7dad-4deb-b246-0ee996b595d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training epoch: 1\n\nTraining loss per 100 training steps: 3.5384016036987305\n\nTraining loss per 100 training steps: 0.6928099407122867\n\nTraining loss per 100 training steps: 0.4401657183223696\n\nTraining loss per 100 training steps: 0.34316632154078974\n\nTraining loss per 100 training steps: 0.2870647047003011\n\nTraining loss per 100 training steps: 0.25085872645209173\n\nTraining loss epoch: 0.2503981324242313\n\nTraining accuracy epoch: 0.7170884029660767\n\nTraining epoch: 2\n\nTraining loss per 100 training steps: 0.07849100232124329\n\nTraining loss per 100 training steps: 0.09188034326428234\n\nTraining loss per 100 training steps: 0.08638632642244225\n\nTraining loss per 100 training steps: 0.08298441354321483\n\nTraining loss per 100 training steps: 0.07989651686569996\n\nTraining loss per 100 training steps: 0.07768682895336085\n\nTraining loss epoch: 0.07755126204215745\n\nTraining accuracy epoch: 0.8533333538476084\n\nTraining epoch: 3\n\nTraining loss per 100 training steps: 0.061504095792770386\n\nTraining loss per 100 training steps: 0.06343790567894973\n\nTraining loss per 100 training steps: 0.06116948165555498\n\nTraining loss per 100 training steps: 0.06029297855753835\n\nTraining loss per 100 training steps: 0.059477455235850483\n\nTraining loss per 100 training steps: 0.05905119367241265\n\nTraining loss epoch: 0.05898059407252796\n\nTraining accuracy epoch: 0.8846677709139803\n\nTraining epoch: 4\n\nTraining loss per 100 training steps: 0.05401049181818962\n\nTraining loss per 100 training steps: 0.04957272619525395\n\nTraining loss per 100 training steps: 0.049710707025444924\n\nTraining loss per 100 training steps: 0.0498685915072594\n\nTraining loss per 100 training steps: 0.04959230659125452\n\nTraining loss per 100 training steps: 0.049230402298346014\n\nTraining loss epoch: 0.049257152565712364\n\nTraining accuracy epoch: 0.9012867686820653\n\nTraining epoch: 5\n\nTraining loss per 100 training steps: 0.03383743017911911\n\nTraining loss per 100 training steps: 0.04293222769652263\n\nTraining loss per 100 training steps: 0.04281548973154369\n\nTraining loss per 100 training steps: 0.04307165220874885\n\nTraining loss per 100 training steps: 0.043137697660900705\n\nTraining loss per 100 training steps: 0.04308864099165042\n\nTraining loss epoch: 0.04312364644857927\n\nTraining accuracy epoch: 0.9131665050614193\n\nTraining epoch: 6\n\nTraining loss per 100 training steps: 0.04185590147972107\n\nTraining loss per 100 training steps: 0.035578676877488\n\nTraining loss per 100 training steps: 0.03665773829083834\n\nTraining loss per 100 training steps: 0.03727792056766261\n\nTraining loss per 100 training steps: 0.03758881605251174\n\nTraining loss per 100 training steps: 0.03790387020868337\n\nTraining loss epoch: 0.037854012305557606\n\nTraining accuracy epoch: 0.9223478678570768\n\nTraining epoch: 7\n\nTraining loss per 100 training steps: 0.037449128925800323\n\nTraining loss per 100 training steps: 0.03336609638120869\n\nTraining loss per 100 training steps: 0.03366411893410766\n\nTraining loss per 100 training steps: 0.033707255571833086\n\nTraining loss per 100 training steps: 0.033455470850937384\n\nTraining loss per 100 training steps: 0.0335352592720481\n\nTraining loss epoch: 0.0335212411177502\n\nTraining accuracy epoch: 0.9306323263805102\n\nTraining epoch: 8\n\nTraining loss per 100 training steps: 0.03873296454548836\n\nTraining loss per 100 training steps: 0.02987069039061518\n\nTraining loss per 100 training steps: 0.028824250487175154\n\nTraining loss per 100 training steps: 0.029367305681827258\n\nTraining loss per 100 training steps: 0.029636542682078413\n\nTraining loss per 100 training steps: 0.02967883146064962\n\nTraining loss epoch: 0.02964302778214394\n\nTraining accuracy epoch: 0.9388598896890916\n\nTraining epoch: 9\n\nTraining loss per 100 training steps: 0.031449656933546066\n\nTraining loss per 100 training steps: 0.0259206375605104\n\nTraining loss per 100 training steps: 0.025016302632084535\n\nTraining loss per 100 training steps: 0.025690758050082134\n\nTraining loss per 100 training steps: 0.025957592324054153\n\nTraining loss per 100 training steps: 0.026227847444350846\n\nTraining loss epoch: 0.026243956201333945\n\nTraining accuracy epoch: 0.9454375839085156\n\nTraining epoch: 10\n\nTraining loss per 100 training steps: 0.011127416044473648\n\nTraining loss per 100 training steps: 0.022518829691528092\n\nTraining loss per 100 training steps: 0.022703044760545986\n\nTraining loss per 100 training steps: 0.022699495953851165\n\nTraining loss per 100 training steps: 0.022873029830468416\n\nTraining loss per 100 training steps: 0.023288752603904155\n\nTraining loss epoch: 0.023317405434647505\n\nTraining accuracy epoch: 0.9514280087109872\n\nValidation loss per 100 evaluation steps: 0.09889696538448334\n\nValidation loss per 100 evaluation steps: 0.06621498103705373\n\nValidation loss per 100 evaluation steps: 0.064057192450102\n\nValidation Loss: 0.06468214777978284\n\nValidation Accuracy: 0.8963840482047295\n\n                 precision    recall  f1-score   support\n\n\n\n   allergy_name       0.80      0.80      0.80       711\n\n         cancer       0.74      0.75      0.74      2712\n\nchronic_disease       0.77      0.80      0.78      6781\n\n      treatment       0.79      0.82      0.80      8836\n\n\n\n      micro avg       0.77      0.80      0.79     19040\n\n      macro avg       0.77      0.79      0.78     19040\n\n   weighted avg       0.77      0.80      0.79     19040\n\n\n"
        },
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'allergy_name': {'precision': 0.7994350282485876,\n",
              "  'recall': 0.7960618846694796,\n",
              "  'f1-score': 0.7977448907681467,\n",
              "  'support': 711},\n",
              " 'cancer': {'precision': 0.7352834958468761,\n",
              "  'recall': 0.7507374631268436,\n",
              "  'f1-score': 0.7429301222404671,\n",
              "  'support': 2712},\n",
              " 'chronic_disease': {'precision': 0.7712614155251142,\n",
              "  'recall': 0.7970800766848547,\n",
              "  'f1-score': 0.7839582275727028,\n",
              "  'support': 6781},\n",
              " 'treatment': {'precision': 0.7866013071895425,\n",
              "  'recall': 0.8172249886826618,\n",
              "  'f1-score': 0.801620781527531,\n",
              "  'support': 8836},\n",
              " 'micro avg': {'precision': 0.774370709382151,\n",
              "  'recall': 0.7997899159663866,\n",
              "  'f1-score': 0.7868750807389226,\n",
              "  'support': 19040},\n",
              " 'macro avg': {'precision': 0.7731453117025301,\n",
              "  'recall': 0.79027610329096,\n",
              "  'f1-score': 0.7815635055272119,\n",
              "  'support': 19040},\n",
              " 'weighted avg': {'precision': 0.7743077707365583,\n",
              "  'recall': 0.7997899159663866,\n",
              "  'f1-score': 0.7868258968277343,\n",
              "  'support': 19040}}"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "E-5GDIiSivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "cls_rep=val_ner(testing_loader_g)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:52:29.720497Z",
          "iopub.status.busy": "2024-04-21T17:52:29.720072Z",
          "iopub.status.idle": "2024-04-21T17:52:59.492772Z",
          "shell.execute_reply": "2024-04-21T17:52:59.491683Z"
        },
        "papermill": {
          "duration": 29.813342,
          "end_time": "2024-04-21T17:52:59.495217",
          "exception": false,
          "start_time": "2024-04-21T17:52:29.681875",
          "status": "completed"
        },
        "tags": [],
        "id": "MKe-kF31ivH0",
        "outputId": "fe41f25e-ba94-405a-9ab7-26dfa8bfa8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Validation loss per 100 evaluation steps: 0.07050216197967529\n\nValidation loss per 100 evaluation steps: 0.06034811463502079\n\nValidation loss per 100 evaluation steps: 0.06324942483664003\n\nValidation Loss: 0.06429393810500937\n\nValidation Accuracy: 0.8957069640996693\n\n                 precision    recall  f1-score   support\n\n\n\n   allergy_name       0.80      0.80      0.80       711\n\n         cancer       0.74      0.75      0.74      2712\n\nchronic_disease       0.77      0.80      0.78      6781\n\n      treatment       0.79      0.82      0.80      8836\n\n\n\n      micro avg       0.77      0.80      0.79     19040\n\n      macro avg       0.77      0.79      0.78     19040\n\n   weighted avg       0.77      0.80      0.79     19040\n\n\n"
        }
      ],
      "id": "MKe-kF31ivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "# These are just to fetch f1-scores\n",
        "\n",
        "clsrep_t123comb = pd.DataFrame(cls_rep).transpose()\n",
        "# Convert dictionary to DataFrame\n",
        "cls_rep_df = pd.DataFrame(cls_rep)\n",
        "# Filter out rows containing \"micro avg\" and \"macro avg\"\n",
        "filtered_df = cls_rep_df[~cls_rep_df.index.isin(['micro avg', 'macro avg'])]\n",
        "# Extract the F1-score column\n",
        "f1_scores = filtered_df.loc['f1-score']\n",
        "# Optionally, convert the f1_scores to numeric if it's in string format\n",
        "f1_scores_t1t2t3combined = pd.to_numeric(f1_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:52:59.567048Z",
          "iopub.status.busy": "2024-04-21T17:52:59.566741Z",
          "iopub.status.idle": "2024-04-21T17:52:59.574513Z",
          "shell.execute_reply": "2024-04-21T17:52:59.573579Z"
        },
        "papermill": {
          "duration": 0.04551,
          "end_time": "2024-04-21T17:52:59.576484",
          "exception": false,
          "start_time": "2024-04-21T17:52:59.530974",
          "status": "completed"
        },
        "tags": [],
        "id": "lD5eMw6DivH0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lD5eMw6DivH0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing performance | Continual Learning vs Learning all tasks together\n",
        "\n",
        "series_list = [\n",
        "    f1_scores_t1,\n",
        "    f1_scores_t1t2,\n",
        "    f1_scores_t1t2t3,\n",
        "    f1_scores_t1t2t3combined\n",
        "]\n",
        "\n",
        "# Concatenate the Series\n",
        "f1s = pd.concat(series_list, axis=1)\n",
        "\n",
        "# Rename the columns\n",
        "f1s.columns = [\n",
        "'Performance on the test set of T1',\n",
        "'Performance on the test set of T1 and T2',\n",
        "'Performance on the test set of T1, T2 and T3',\n",
        "'Performance on combined G1+G2+G3'\n",
        "\n",
        "]\n",
        "\n",
        "# Export to CSV\n",
        "f1s.to_csv('f1s.csv', index=False)\n",
        "\n",
        "f1s"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:52:59.718189Z",
          "iopub.status.busy": "2024-04-21T17:52:59.717319Z",
          "iopub.status.idle": "2024-04-21T17:52:59.735048Z",
          "shell.execute_reply": "2024-04-21T17:52:59.734183Z"
        },
        "papermill": {
          "duration": 0.056473,
          "end_time": "2024-04-21T17:52:59.737191",
          "exception": false,
          "start_time": "2024-04-21T17:52:59.680718",
          "status": "completed"
        },
        "tags": [],
        "id": "KsJTmtLgivH1",
        "outputId": "1822f6a6-8f7a-4eda-a20c-4ed1cb556218"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 55,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Performance on the test set of T1</th>\n",
              "      <th>Performance on the test set of T1 and T2</th>\n",
              "      <th>Performance on the test set of T1, T2 and T3</th>\n",
              "      <th>Performance on combined G1+G2+G3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>allergy_name</th>\n",
              "      <td>0.738386</td>\n",
              "      <td>0.824524</td>\n",
              "      <td>0.901130</td>\n",
              "      <td>0.797745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cancer</th>\n",
              "      <td>0.726179</td>\n",
              "      <td>0.793356</td>\n",
              "      <td>0.833593</td>\n",
              "      <td>0.742930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chronic_disease</th>\n",
              "      <td>0.779630</td>\n",
              "      <td>0.805484</td>\n",
              "      <td>0.854898</td>\n",
              "      <td>0.783958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>treatment</th>\n",
              "      <td>0.777369</td>\n",
              "      <td>0.840783</td>\n",
              "      <td>0.881324</td>\n",
              "      <td>0.801621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>micro avg</th>\n",
              "      <td>0.769029</td>\n",
              "      <td>0.820854</td>\n",
              "      <td>0.865570</td>\n",
              "      <td>0.786875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.755391</td>\n",
              "      <td>0.816037</td>\n",
              "      <td>0.867736</td>\n",
              "      <td>0.781564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.768511</td>\n",
              "      <td>0.820849</td>\n",
              "      <td>0.865853</td>\n",
              "      <td>0.786826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Performance on the test set of T1  \\\n",
              "allergy_name                              0.738386   \n",
              "cancer                                    0.726179   \n",
              "chronic_disease                           0.779630   \n",
              "treatment                                 0.777369   \n",
              "micro avg                                 0.769029   \n",
              "macro avg                                 0.755391   \n",
              "weighted avg                              0.768511   \n",
              "\n",
              "                 Performance on the test set of T1 and T2  \\\n",
              "allergy_name                                     0.824524   \n",
              "cancer                                           0.793356   \n",
              "chronic_disease                                  0.805484   \n",
              "treatment                                        0.840783   \n",
              "micro avg                                        0.820854   \n",
              "macro avg                                        0.816037   \n",
              "weighted avg                                     0.820849   \n",
              "\n",
              "                 Performance on the test set of T1, T2 and T3  \\\n",
              "allergy_name                                         0.901130   \n",
              "cancer                                               0.833593   \n",
              "chronic_disease                                      0.854898   \n",
              "treatment                                            0.881324   \n",
              "micro avg                                            0.865570   \n",
              "macro avg                                            0.867736   \n",
              "weighted avg                                         0.865853   \n",
              "\n",
              "                 Performance on combined G1+G2+G3  \n",
              "allergy_name                             0.797745  \n",
              "cancer                                   0.742930  \n",
              "chronic_disease                          0.783958  \n",
              "treatment                                0.801621  \n",
              "micro avg                                0.786875  \n",
              "macro avg                                0.781564  \n",
              "weighted avg                             0.786826  "
            ]
          },
          "metadata": {}
        }
      ],
      "id": "KsJTmtLgivH1"
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"KaggleMasterX/BERT_AllTasks\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-21T17:52:59.812433Z",
          "iopub.status.busy": "2024-04-21T17:52:59.812144Z",
          "iopub.status.idle": "2024-04-21T17:53:17.334759Z",
          "shell.execute_reply": "2024-04-21T17:53:17.333775Z"
        },
        "papermill": {
          "duration": 17.561743,
          "end_time": "2024-04-21T17:53:17.337235",
          "exception": false,
          "start_time": "2024-04-21T17:52:59.775492",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "10d78dc9cfc346d4911476492b3611de"
          ]
        },
        "id": "6d0W6X2UivH1",
        "outputId": "dcabb28d-e529-476a-de02-15c6e0df932b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10d78dc9cfc346d4911476492b3611de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 56,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/KaggleMasterX/BERT_AllTasks/commit/e5d30676398f8ca06deb992fa44da65a45a2aeb5', commit_message='Upload BertForTokenClassification', commit_description='', oid='e5d30676398f8ca06deb992fa44da65a45a2aeb5', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "6d0W6X2UivH1"
    }
  ]
}